---
title: Composio
description: Learn how to connect Tavily to Composio to give your agents powerful web search capabilities.
keywords:
  - Tavily
  - Composio
  - integration
  - web search
  - tools
  - agents
---

## Overview

This guide shows how to integrate Tavily with Composio so your agents can perform high-quality web searches as part of their workflows.

You will:

- Install the required Python packages
- Configure your Tavily and model provider API keys
- Connect Tavily as a Composio tool
- Build a simple agent that calls Tavily via Composio

<Callout kind="info">

This page assumes you already have a Composio account and a Tavily API key. If not, create those first before continuing.

</Callout>

## Prerequisites

- Python 3.9 or later
- A Tavily API key from the [Tavily dashboard](https://app.tavily.com/)
- A model provider API key (for example, [OpenAI](https://platform.openai.com/api-keys))
- A Composio account and workspace

## Step-by-step setup

<Steps>

  <Step title="Install the required packages" icon="package">

Make sure you have the Composio SDK and a compatible LLM client installed in your Python environment.

```bash
pip install composio-core composio-openai openai tavily-python
```

If you use a different LLM provider, install the corresponding Composio integration package instead of `composio-openai`.

  </Step>

  <Step title="Configure your API keys" icon="key">

Set your Tavily and model provider API keys as environment variables, or pass them directly when initializing clients.

```bash
export TAVILY_API_KEY="your-tavily-api-key"
export OPENAI_API_KEY="your-openai-api-key"
```

In Python, the OpenAI and Tavily clients will automatically pick up these environment variables.

  </Step>

  <Step title="Initialize Composio and connect Tavily" icon="plug">

Create a Composio client, connect your agent to Composio, and include Tavily as one of the available tools.

```python
import os

from composio_openai import ComposioToolSet
from openai import OpenAI

# Initialize OpenAI client
openai_client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

# Initialize Composio toolset
toolset = ComposioToolSet()

# Select tools from Composio, including Tavily
tools = toolset.get_tools([
    "tavily_search",   # Tavily web search tool
])
```

At this point, you have access to a `tavily_search` tool you can route to from your agent.

  </Step>

  <Step title="Build an agent that calls Tavily via Composio" icon="workflow">

Wire the Composio tools into your LLM agent so it can call Tavily search when needed.

```python
import json

def run_agent(user_query: str) -> str:
    # Prepare tool definitions for the model
    tool_schemas = toolset.get_tools_for_openai(tools)

    # Call the model with tool/function calling enabled
    response = openai_client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {
                "role": "system",
                "content": "You are an assistant that can use tools to search the web when needed."
            },
            {
                "role": "user",
                "content": user_query,
            },
        ],
        tools=tool_schemas,
        tool_choice="auto",
    )

    message = response.choices[0].message

    # If the model decided to call a tool, execute it via Composio
    if message.tool_calls:
        for tool_call in message.tool_calls:
            args = json.loads(tool_call.function.arguments)
            result = toolset.call_tool(
                tool_call.function.name,
                params=args,
            )

        # Optionally, send tool results back to the model
        followup = openai_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are an assistant that can use tools to search the web when needed."
                },
                {
                    "role": "user",
                    "content": user_query,
                },
                {
                    "role": "assistant",
                    "tool_calls": message.tool_calls,
                    "content": None,
                },
                {
                    "role": "tool",
                    "tool_call_id": message.tool_calls[0].id,
                    "name": message.tool_calls[0].function.name,
                    "content": json.dumps(result),
                },
            ],
        )
        return followup.choices[0].message.content or ""

    # If no tool call was needed, return the model response directly
    return message.content or ""
```

Now, when the model determines that web search is useful, it will call the Tavily search tool through Composio, and you can feed the results back into the model for a final answer.

  </Step>

</Steps>

## Full example: answering questions with Tavily via Composio

The following example shows a minimal end-to-end script that:

1. Initializes Composio with Tavily search
2. Sets up an OpenAI client
3. Lets the model decide when to call Tavily
4. Returns a grounded answer to the user

<CodeGroup tabs="Python">

```python
import os
import json

from composio_openai import ComposioToolSet
from openai import OpenAI

# Load API keys from environment
openai_client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

# Initialize Composio toolset
toolset = ComposioToolSet()

# Pick Tavily search as an available tool
tools = toolset.get_tools([
    "tavily_search",
])

tool_schemas = toolset.get_tools_for_openai(tools)

def answer_question(query: str) -> str:
    system_prompt = (
        "You are a research assistant. "
        "Use the web search tool when the question requires up-to-date or factual information."
    )

    # First call: let the model decide if it wants to use Tavily
    response = openai_client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query},
        ],
        tools=tool_schemas,
        tool_choice="auto",
    )

    message = response.choices[0].message

    if message.tool_calls:
        # Execute all requested tools
        tool_results = []
        for tool_call in message.tool_calls:
            args = json.loads(tool_call.function.arguments)
            result = toolset.call_tool(
                tool_call.function.name,
                params=args,
            )
            tool_results.append(
                {
                    "id": tool_call.id,
                    "name": tool_call.function.name,
                    "result": result,
                }
            )

        # Second call: send tool outputs back for a final grounded answer
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query},
            {
                "role": "assistant",
                "tool_calls": message.tool_calls,
                "content": None,
            },
        ]

        for tr in tool_results:
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tr["id"],
                    "name": tr["name"],
                    "content": json.dumps(tr["result"]),
                }
            )

        final = openai_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
        )
        return final.choices[0].message.content or ""

    # No tool call needed: just return the model output
    return message.content or ""


if __name__ == "__main__":
    print(answer_question("What are the latest developments in quantum computing?"))
```

</CodeGroup>

<Callout kind="tip">

You can refine when the model should use Tavily by updating the `system_prompt` to emphasize freshness, citations, or specific domains.

</Callout>

## Additional use cases

You can use Tavily with Composio in many workflows, for example:

- **Research agents** that gather up-to-date information before drafting reports or briefs
- **Customer support copilots** that look up current documentation, pricing, or status pages
- **Monitoring and alerts** that periodically search for new information about competitors, vulnerabilities, or regulations
- **Content creation assistants** that pull in recent sources, statistics, and citations for blogs or social content
- **Compliance and due diligence bots** that research entities across the public web
- **Domain-specific scouts** focused on topics like AI safety, finance, or healthcare news

<ExpandableGroup>

  <Expandable title="Optimizing search quality" default-open="false">

You can tune Tavily search behavior by:

- Narrowing or broadening the search depth via query parameters in the tool call
- Instructing the model to ask follow-up questions before searching when the query is ambiguous
- Combining multiple Tavily calls to cross-check results from different angles

  </Expandable>

  <Expandable title="Combining Tavily with other Composio tools" default-open="false">

Tavily works well alongside other Composio tools, for example:

- Use Tavily to gather context, then send it to a document storage or vector database tool
- Trigger outbound notifications (email, Slack) based on Tavily search findings
- Chain Tavily with calendar or task tools so research leads directly to scheduled actions

  </Expandable>

</ExpandableGroup>

<Columns cols={2}>

  <Card
    title="Tavily Quickstart"
    href="/tavily/quickstart"
    icon="rocket"
    cta="Get started"
  >

Get up and running with the Tavily API in a few minutes, including authentication and basic examples.

  </Card>

  <Card
    title="API Reference"
    href="/tavily/api-reference/introduction"
    icon="code"
    cta="Explore API"
  >

See all Tavily endpoints, parameters, and response formats you can use in your Composio tools.

  </Card>

</Columns>