---
title: Best Practices for Crawl
description: Learn when and how to use Tavily's Crawl API, in plain language.
---

## Crawl vs. Map (simple rule)

Use **Crawl** when you care about **page content**.
Use **Map** when you care about **URLs and structure**.

### Use Crawl when you need:

- The actual text (and optional images) from pages
- Deeper analysis of content
- Data for search or RAG

### Use Map when you need:

- A quick list of URLs
- To see how a site is structured
- Paths to later pass into Crawl

---

## Common Crawl setups

Below are a few beginner-friendly patterns you can copy and adjust.

### 1. Blog or changelog section

```json
{
  "url": "https://example.com",
  "max_depth": 2,
  "max_breadth": 50,
  "limit": 200,
  "select_paths": ["/blog/.*", "/changelog/.*"],
  "exclude_paths": ["/private/.*", "/admin/.*"]
}
```

Use this when you want posts or release notes, not the whole site.

### 2. Documentation for RAG

```json
{
  "url": "https://docs.example.com",
  "max_depth": 2,
  "extract_depth": "advanced",
  "include_images": true
}
```

Use this when you want high-quality content to feed into an LLM or search index.

### 3. Fast, shallow crawl

```json
{
  "url": "https://api.example.com",
  "max_depth": 1,
  "max_breadth": 100,
  "extract_depth": "basic"
}
```

Use this when you want a quick snapshot of a small part of a site.

### 4. Known URL patterns

```json
{
  "url": "https://example.com",
  "max_depth": 1,
  "select_paths": ["/docs/.*", "/guides/.*"],
  "exclude_paths": ["/admin/.*"]
}
```

Use this when you already know which folders matter.

---

## Performance tips

- **Start shallow:** begin with `max_depth: 1` and small `max_breadth`.
- **Limit scope:** use `select_paths` / `exclude_paths` so you don’t crawl the whole site.
- **Choose extract depth wisely:**
  - `basic` for simple pages
  - `advanced` only when you truly need richer extraction

<Callout kind="tip">

If a crawl feels slow or expensive:

- Lower `max_depth` and `max_breadth`.
- Add more specific `select_paths`.
- Switch from `advanced` to `basic` extract depth.

</Callout>

---

## Safe and polite crawling

- Respect `robots.txt` and site policies.
- Avoid crawling sensitive or private paths.
- Add auth only where you are allowed to (internal docs, paywalled help centers, etc.).

Example for an internal help center:

```json
{
  "url": "https://help.example.com",
  "max_depth": 2,
  "select_domains": ["^help\\.example\\.com$"],
  "exclude_paths": ["/admin/.*", "/billing/.*"]
}
```

---

## Quick checklist

Before you run a big crawl, check:

1. Do I really need this depth and breadth?
2. Have I focused on the right paths?
3. Am I respecting the site’s rules and rate limits?

If the answer is yes, you’re ready to run Crawl and plug the results into your pipeline or RAG system.